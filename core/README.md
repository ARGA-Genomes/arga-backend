The arga-core crate provides core functionality to all tools and servers that need database, search index, or CRDT integration. This is effectively the synchronisation point between ARGA projects and the backend web service.

## Consumers
The applications and tools that leverage the core crate
- backend-server
- backend-workers
- search-indexer
- oplogger

## Migrations
The database schema is *declarative* which means that the single source of truth is defined in `core/schema.sql`. All changes to the schema should be made in that file.
Once you have your final desired schema generate a versioned migration with the atlasgo.io CLI tool by running in the `core` directory
``` sh
atlas migrate diff create_my_table --env arga
```

There is a configuration for atlas at `core/atlas.hcl` which will use your existing environment variables to connect to the temporary migrator database (`MIGRATOR_DATABASE_URL` in the .env file) and generate a diff between the desired `schema.sql` file migrating from the migrations already defined in the `migrations` folder.

The generated migration file should be reviewed and modified if it needs tweaking.

This is called a 'versioned migration' process according to atlas. Basically it generates imperative migrations in the `migrations` folder like a traditional schema migration tool would, except that those files are generated by doing a diff between the 'last' state of the schema to the new one.
Which gives us the best of both worlds, declaritive schema management while still being able to review and approve individual changes in pull requests.

Note however that this process does not create a `down` version of the migration. This is by design. We instead revert schemas the same way we would revert a merge or commit, by adding another migration to undo what has been done.

### Running migrations
To run a migration make sure that the `.env` file in the repository root has an entry for the `DATABASE_URL` pointing to your postgres installation. For example, `DATABASE_URL=postgres://gsterjov@localhost/arga`. You can then run all pending migrations bringing your database up to date by executing

``` sh
atlas migrate apply --env arga
```

The atlas-cli is well documented, feel free to append `--help` to any command to find all the functionality it offers.

### Post migration
Because we no longer use diesel-cli to migrate our schema we will instead need to manually regenerate the `schema.rs` file for a strongly typed definition of the schema in our Rust codebases.

`schema.rs` is a rust file using the diesel DSL to define all tables, types, relations, and types. Its the core file used to power the diesel query builder in the backend and related tools.
A manually maintained file called `src/schema_gnl.rs` sits alongside the schema file. It uses the same DSL to define views and materialized views so they can be used as regular tables in the backend and related tools. We need to manually maintain these as diesel-cli does not have support for generating schema definitions for views.

If you create or modify views be sure to update the `schema_gnl.rs` file to reflect the new definition otherwise we will run into errors with schema mismatches.

To update the schema run the following making sure to direct the output to the rust schema file

``` sh
# bash
diesel print-schema > src/schema.rs

# nushell
diesel print-schema | save -f src/schema.rs
```

It is important to note however that diesel print-schema will connect to your *development* database to analyse the schema. So be sure that what you have on your development database is consistent with the codebase and the schema.sql file.
Because we heavily practice continuous integration this should not be much of an issue as the schema won't drift much during feature development.
